Code for my thesis "Clustering Human and Robot Web Traffic Data"

In this thesis, we try to understand the web robot traffic on the Web by comparing it to the human traffic using clustering techniques and cluster analysis. We experiment with the k-means algorithm and we cluster sessions that derived from real log files of a server. In our experiments, we use simple features that have been previously adopted by other studies and are extracted from the server log files, semantic features derived by performing semantic analysis on the text of the websites visited, and a combination of both types of features. We use two csv files, one includes all the sessions with simple features and the other all the sessions with semantic features (indicative files are the simple_features.csv and semantic_features.csv files in this repository).
  
In any case, first we execute the k-means algorithm for k=2 to cluster only the labeled sessions and then we find the optimal k for each case by studying the silhouette coefficient. For the data we used, the optimal k for the labeled sessions with simple features is 5, for the labeled sessions with semantic features is 6 and for the labeled sessions with simple and semantic features is 10. Lastly, in any case, we plot the distribution of each cluster for specific features against the total of labeled sessions and show the maximum, the minimum and range of the chosen features.
